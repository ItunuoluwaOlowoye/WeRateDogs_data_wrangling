{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727e3766",
   "metadata": {},
   "source": [
    "# Wrangle Report: WeRateDogs Twitter Archive\n",
    "This report documents the data wrangling of tweets by *WeRateDogs*. This report is to aid reproducibility of the data wrangling section of the analysis.\n",
    "\n",
    "The data wrangling process was carried out in a Jupyter notebook using Python. All packages needed were imported into the notebook. These packages include *pandas (alias **pd**), numpy (alias **np**), seaborn, requests, os, json, tweepy, matplotlib, PIL, io, timeit, and dotenv.* Any of these packages not initially available in the environment were installed using `pip install --packagename`.\n",
    "## Data Gathering\n",
    "Data for this analysis were gathered from three different sources.\n",
    "#### 1. Locally available Twitter archive\n",
    "Udacity provided the Twitter archive and it can be downloaded [here](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv). The data is a csv file which was read into a dataframe using `pd.read_csv('filepath')`.\n",
    "#### 2. Image prediction file stored on the web in Udacity's server\n",
    "The file was accessed from the web using the *requests* library. First, a folder to download the file into was created within the same repository as the notebook. Then, the file was downloaded and stored in the folder. Finally, the file (.tsv extension) was read into another dataframe using `pd.read_csv('filepath', sep='\\t')`.\n",
    "#### 3. Data pulled from the Twitter API\n",
    "Additional data about the tweets provided in the archive was pulled from the Twitter API using the *tweepy* package. The *dotenv* package was used to get the keys needed for authentication from a *.env* file. The *.env* file was created in the same repository as the notebook using VS Code. The file stores the keys as variables that can be called using `os.getenv('key_here')`.\n",
    "\n",
    "An array of the tweet ids from the archive was created. Data from every tweet pulled was written and stored into a json file. A `try-except` code was used to bypass any errors that could result as a result of attempting to pull data from a deleted tweet. This file was created as a *JSON lines* format file and was read into a third dataframe using `pd.read_json('filepath', lines=True)`.\n",
    "## Data Assessment\n",
    "The three dataframes were assessed visually and programmatically to understand the dataset and document data cleaning procedures that should be carried out, if any. The functions called to assess the dataframes include: `.info()`, `.describe()`, `.duplicated()`, and `value_counts()`.\n",
    "\n",
    "The assessment identified different data quality and tidiness issues that are documented below.\n",
    "### Quality\n",
    "##### Completeness:\n",
    "* No column for the most accurate dog breed prediction.\n",
    "\n",
    "##### Validity:\n",
    "* \"None\" should be default null values.\n",
    "* Timestamps should be datetime format, not strings.\n",
    "* Ids should be strings, not numerics.\n",
    "\n",
    "##### Accuracy:\n",
    "* Some names are inaccurate.\n",
    "* Duplicates are present in form of retweets.\n",
    "\n",
    "##### Consistency:\n",
    "* Primary and foreign keys shoud have consistent names across dataframes.\n",
    "* Ratings should be on the same scale.\n",
    "\n",
    "### Tidiness\n",
    "* Dog stage observations should be in **one** column, not four.\n",
    "* Certain columns in a dataframe should be merged with another dataframe.\n",
    "\n",
    "## Data Cleaning\n",
    "All data quality and tidiness issues identified while assessing the data were cleaned using the *Define-Code-Test* framework on copies of the original data.\n",
    "The cleaning process involves:\n",
    "* Creating a new column that selects the dog breed with the maximum prediction confidence using `np.select`.\n",
    "* Replacing \"None\" values with null `np.nan` values.\n",
    "* Converting timestamps to datetime format using `pd.to_datetime`.\n",
    "* Converting to strings using `astype(str)`.\n",
    "* Replacing inaccurate values with null values using `.replace()`\n",
    "* Removing retweets using `.isnull()` and `.notnull()` functions.\n",
    "* Renaming inconsistent columns using `.rename()`.\n",
    "* Selecting observations on the same rating scale using `.query()`.\n",
    "* Joining columns and extracting data needed into one column using `.agg()` and `str.extract()` functions.\n",
    "* Merging dataframes using `pd.merge()`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:udacity_env] *",
   "language": "python",
   "name": "conda-env-udacity_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
